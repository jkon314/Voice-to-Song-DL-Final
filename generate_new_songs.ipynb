{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import utils\n",
    "import numpy as np\n",
    "from Vocoder import Vocoder\n",
    "from Model import Model\n",
    "import torchaudio as ta\n",
    "from resemblyzer import preprocess_wav, VoiceEncoder\n",
    "\n",
    "\n",
    "#load Model\n",
    "myModel = Model()\n",
    "\n",
    "#load desired model with trained weights\n",
    "myModel.load_state_dict(torch.load('model_final_e2_w_style.pt'))\n",
    "\n",
    "# Create this folder manually if you don't have it\n",
    "temp_dir = \"./temp\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Song into vocals and instrumentals\n",
    "\n",
    "from audio_separator.separator import Separator\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the Separator class (with optional configuration properties, below)\n",
    "separator = Separator(output_format=\"mp3\", output_dir=temp_dir)\n",
    "\n",
    "# Load a machine learning model (if unspecified, defaults to 'model_mel_band_roformer_ep_3005_sdr_11.4360.ckpt')\n",
    "separator.load_model(model_filename='UVR-MDX-NET-Inst_HQ_3.onnx')\n",
    "\n",
    "song_path = r\"C:\\Users\\baseb\\Downloads\\Not Like Us.mp3\" #put the path to your song here. r here for raw string to prevent escape characters when copying fpath\n",
    "\n",
    "\n",
    "output_files = separator.separate(song_path,)\n",
    "print(f\"Separation complete! Output file(s): {' '.join(output_files)}\")\n",
    "\n",
    "\n",
    "print('Successfully Separated Vocals and Instrumentals!')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocal_file = os.path.join(temp_dir,output_files[0])\n",
    "instrumental_file = os.path.join(temp_dir,output_files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 47289])\n"
     ]
    }
   ],
   "source": [
    "#make the spec of song \n",
    "#optionally start here if you already have the song separated\n",
    "\n",
    "\n",
    "path_to_song = vocal_file\n",
    "transform = ta.transforms.Spectrogram(510)\n",
    "\n",
    "vocals = ta.load(path_to_song)\n",
    "mono = torch.mean(vocals[0],0,False)\n",
    "\n",
    "spec = transform(mono)\n",
    "spec = torch.unsqueeze(spec,0)\n",
    "print(spec.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the voice encoder model on cuda in 0.02 seconds.\n",
      "torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "#Make style of song\n",
    "wav = preprocess_wav(path_to_song)\n",
    "encoder = VoiceEncoder()\n",
    "style = encoder.embed_utterance(wav)\n",
    "\n",
    "style = torch.tensor(style)\n",
    "style = torch.unsqueeze(style,0)\n",
    "print(style.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 256)\n",
      "(256,)\n",
      "torch.Size([1, 256])\n"
     ]
    }
   ],
   "source": [
    "#Get the input voice's style embedding\n",
    "\n",
    "#path_to_targets = [r\"C:\\Users\\baseb\\OneDrive\\Desktop\\Spring 2024 Classes\\DL\\Project\\Voice-to-Song-DL-Final\\Tracks_2\\vocals_chunks\\01 Cherry_(Vocals)_UVR-MDX-NET-Inst_HQ_3-chunk-42.mp3\",\n",
    "# r\"C:\\Users\\baseb\\OneDrive\\Desktop\\Spring 2024 Classes\\DL\\Project\\Voice-to-Song-DL-Final\\Tracks_2\\vocals_chunks\\01 Cherry_(Vocals)_UVR-MDX-NET-Inst_HQ_3-chunk-39.mp3\",\n",
    "# r\"C:\\Users\\baseb\\OneDrive\\Desktop\\Spring 2024 Classes\\DL\\Project\\Voice-to-Song-DL-Final\\Tracks_2\\vocals_chunks\\01 Cherry_(Vocals)_UVR-MDX-NET-Inst_HQ_3-chunk-40.mp3\",\n",
    "# r\"C:\\Users\\baseb\\OneDrive\\Desktop\\Spring 2024 Classes\\DL\\Project\\Voice-to-Song-DL-Final\\Tracks_2\\vocals_chunks\\01 Cherry_(Vocals)_UVR-MDX-NET-Inst_HQ_3-chunk-41.mp3\"]\n",
    "path_to_targets = [r\"C:\\Users\\baseb\\OneDrive\\Desktop\\Spring 2024 Classes\\DL\\Project\\Voice-to-Song-DL-Final\\Tracks_2\\Speech_examples\\Jerome_style.m4a\"\n",
    ",r\"C:\\Users\\baseb\\OneDrive\\Desktop\\Spring 2024 Classes\\DL\\Project\\Voice-to-Song-DL-Final\\Tracks_2\\Speech_examples\\style_jerome2.m4a\"]\n",
    "embeddings = []\n",
    "\n",
    "style_name = 'jerome' #put a name here to denote the style of the singer\n",
    "\n",
    "for path in path_to_targets:\n",
    "    _wav = preprocess_wav(path)\n",
    "    t = encoder.embed_utterance(_wav)\n",
    "    embeddings.append(t)\n",
    "\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "print(embeddings.shape)\n",
    "embeddings = np.mean(embeddings,0)\n",
    "print(embeddings.shape)\n",
    "\n",
    "target = torch.tensor(embeddings)\n",
    "target = torch.unsqueeze(target,0)\n",
    "\n",
    "print(target.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel.eval()\n",
    "outputs = myModel.forward((spec,style,target))\n",
    "\n",
    "waveforms = outputs[3] #get the postnet output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 47289])\n",
      "torch.Size([1, 12058440])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import soundfile as sf\n",
    "from Vocoder import Vocoder\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example batch of magnitude spectrograms of shape (N, 80, T)\n",
    "    \n",
    "    \n",
    "    magnitude_spectrogram_batch = torch.detach(waveforms)\n",
    "    magnitude_spectrogram_batch= magnitude_spectrogram_batch.to('cuda')\n",
    "    print(magnitude_spectrogram_batch.shape)\n",
    "    # Instantiate Vocoder\n",
    "    vocoder = Vocoder(n_fft=510, hop_length=255, win_length=510, num_iters=50)\n",
    "\n",
    "    # Output directory to save reconstructed waveforms\n",
    "    output_dir = temp_dir\n",
    "\n",
    "    # Reconstruct waveforms from the batch of magnitude spectrograms\n",
    "    reconstructed_waveforms = vocoder.reconstruct(magnitude_spectrogram_batch, output_dir, sample_rate=44100,save = True)\n",
    "\n",
    "    # Print the shape of the reconstructed waveforms\n",
    "    print(reconstructed_waveforms.shape)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import overlay_audio\n",
    "\n",
    "path_to_new_voice = os.path.join(output_dir,'reconstructed_0.wav')\n",
    "\n",
    "\n",
    "\n",
    "out_path_and_file_name = './jerome_drake_style/Song Outputs/notLikeUs_Jerome.mp3' #put your output path and file name here example: ./jerome_drake_style/Song Outputs/rap_god_jerome.mp3\n",
    "\n",
    "overlay_audio(path_to_new_voice,instrumental_file,out_path_and_file_name)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs7643-finalproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
